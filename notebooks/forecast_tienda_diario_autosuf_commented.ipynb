{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcb3f0a",
   "metadata": {},
   "source": [
    "# Pron√≥stico de ventas diarias por Tienda\n",
    "\n",
    "* **Dataset**: `ventas_por_tienda_dia.parquet` ‚Äì cada fila ya es la venta agregada de una tienda en una fecha.\n",
    "* **Modelos**:\n",
    "  * **LightGBM** multiserie con `skforecast` (versi√≥n¬†0.15.x)\n",
    "  * **Regresi√≥n lineal** (baseline)\n",
    "\n",
    "El cuaderno:\n",
    "\n",
    "1. Lee el parquet con PySpark (sin agregaciones).\n",
    "2. Completa huecos del calendario y¬†muestra los d√≠as rellenados.\n",
    "3. Genera variables de calendario.\n",
    "4. Entrena, valida y calcula MAE/RMSE de ambos modelos.\n",
    "5. Maneja √≠ndices, tipos y columnas duplicadas para evitar errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94497ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias si hace falta\n",
    "# %pip install pyspark pandas numpy lightgbm scikit-learn skforecast==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8337c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01841f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder.appName('ForecastTiendaDiario').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145d6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = 'ventas_por_tienda_dia.parquet'  # archivo local\n",
    "START_DATE   = '2022-01-02'\n",
    "END_DATE     = '2025-02-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60b19d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas le√≠das: 194168\n",
      "+----------+------+---------+\n",
      "|FechaVenta|Tienda|PrecioVta|\n",
      "+----------+------+---------+\n",
      "|2022-01-02|   203|  1055990|\n",
      "|2022-01-03|   203|  1233800|\n",
      "|2022-01-04|   203|  1512980|\n",
      "+----------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 ¬∑ Leer parquet ya agregado (Tienda-diario)\n",
    "df = (spark.read.parquet(PARQUET_PATH)\n",
    "         .filter((F.col('FechaVenta') >= F.lit(START_DATE)) & (F.col('FechaVenta') <= F.lit(END_DATE)))\n",
    "         .select('FechaVenta', 'Tienda', 'PrecioVta'))\n",
    "\n",
    "print(\"Filas le√≠das:\", df.count())\n",
    "df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e344bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape matriz ancha: (1150, 191)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2 ¬∑ Pivot: columnas = Tienda\n",
    "pivot = (df.groupBy('FechaVenta')\n",
    "           .pivot('Tienda')\n",
    "           .agg(F.first('PrecioVta'))\n",
    "           .fillna(0)\n",
    "           .orderBy('FechaVenta'))\n",
    "\n",
    "pivot_pd = pivot.toPandas()\n",
    "pivot_pd['FechaVenta'] = pd.to_datetime(pivot_pd['FechaVenta'])\n",
    "pivot_pd = pivot_pd.sort_values('FechaVenta')\n",
    "y_wide = pivot_pd.set_index('FechaVenta').astype('float32')\n",
    "del pivot_pd\n",
    "print(\"Shape matriz ancha:\", y_wide.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b82b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se rellenaron 4 d√≠as faltantes.\n",
      "Primeros huecos: [datetime.date(2023, 1, 1), datetime.date(2023, 12, 25), datetime.date(2024, 1, 1), datetime.date(2025, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3 ¬∑ Completar calendario diario y mostrar huecos\n",
    "idx_full = pd.date_range(start=y_wide.index.min(), end=y_wide.index.max(), freq='D')\n",
    "missing = idx_full.difference(y_wide.index)\n",
    "print(f\"Se rellenaron {len(missing)} d√≠as faltantes.\")\n",
    "if len(missing):\n",
    "    print('Primeros huecos:', missing[:10].date.tolist())\n",
    "y_wide = y_wide.reindex(idx_full, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51190175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4 ¬∑ Split y ex√≥genos\n",
    "train_end = pd.Timestamp('2023-12-31')\n",
    "valid_end = pd.Timestamp('2024-09-30')\n",
    "\n",
    "y_train = y_wide.loc[:train_end]\n",
    "y_valid = y_wide.loc[train_end + pd.Timedelta(days=1): valid_end]\n",
    "\n",
    "def calendar(idx):\n",
    "    cal = pd.DataFrame(index=idx)\n",
    "    cal['dow'] = idx.dayofweek.astype('int8')\n",
    "    cal['weekend'] = (cal['dow']>=5).astype('int8')\n",
    "    cal['day'] = idx.day.astype('int8')\n",
    "    cal['month'] = idx.month.astype('int8')\n",
    "    cal['year'] = idx.year.astype('int16')\n",
    "    cal['weekofyear'] = idx.isocalendar().week.astype('int8')\n",
    "    return cal\n",
    "\n",
    "X_train = calendar(y_train.index)\n",
    "X_valid = calendar(y_valid.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396a3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MAESTRIA_CIENCIAS_DE_LOS_DATOS\\SEMESTRE 2025-I\\Proyecto_final\\env_vision\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1066\n",
      "[LightGBM] [Info] Number of data points in the train set: 136565, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 2640374.094036\n",
      "Columnas comunes: 191\n",
      "LightGBM VALID  MAE : 1172365.12 | RMSE : 1665904.65\n"
     ]
    }
   ],
   "source": [
    "# 5 ¬∑ LightGBM multiserie\n",
    "from lightgbm import LGBMRegressor\n",
    "from skforecast.recursive import ForecasterRecursiveMultiSeries\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "lgb = LGBMRegressor(objective='regression', n_estimators=600, learning_rate=0.05,\n",
    "                    subsample=0.8, random_state=42, n_jobs=-1)\n",
    "fc  = ForecasterRecursiveMultiSeries(regressor=lgb, lags=[1,7,14])\n",
    "fc.fit(series=y_train, exog=X_train)        # y_train mantiene columnas INT\n",
    "\n",
    "# ‚îÄ‚îÄ PREDICCI√ìN VALID  (sin levels=‚Ä¶) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "pred_long = fc.predict(\n",
    "    steps = len(y_valid),\n",
    "    exog  = X_valid        # ‚Üê sin levels\n",
    ")\n",
    "\n",
    "# 1) Si viene largo ['level', 'pred']  ‚ûú  pivot a ancho\n",
    "if list(pred_long.columns) == ['level', 'pred']:\n",
    "    pred = (pred_long\n",
    "             .pivot(columns='level', values='pred')\n",
    "             .astype('float32'))\n",
    "    pred.index = y_valid.index\n",
    "else:\n",
    "    if isinstance(pred_long.columns, pd.MultiIndex):\n",
    "        pred = pred_long.groupby(level=0, axis=1).first()\n",
    "    else:\n",
    "        pred = pred_long\n",
    "\n",
    "# 2) Normalizar nombres y alinear\n",
    "pred.columns    = pred.columns.astype(str)\n",
    "y_valid.columns = y_valid.columns.astype(str)\n",
    "common = y_valid.columns.intersection(pred.columns)\n",
    "print(\"Columnas comunes:\", len(common))   # ahora > 0\n",
    "\n",
    "pred   = pred[common]\n",
    "y_eval = y_valid[common]\n",
    "\n",
    "# 3) M√©tricas\n",
    "mae = mean_absolute_error(y_eval.values.flatten(),\n",
    "                          pred.values.flatten())\n",
    "\n",
    "mse = mean_squared_error(y_eval.values.flatten(),\n",
    "                         pred.values.flatten())\n",
    "rmse = np.sqrt(mse)              # ‚àöMSE  ‚Üí  RMSE\n",
    "\n",
    "print(f\"LightGBM VALID  MAE : {mae :.2f} | RMSE : {rmse :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675deec",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Interpretaci√≥n de resultados\n",
    "\n",
    "| Modelo | MAE (VALID) | RMSE (VALID) |\n",
    "|--------|-------------|--------------|\n",
    "| **LightGBM** | **1‚ÄØ172‚ÄØ365** | **1‚ÄØ665‚ÄØ905** |\n",
    "| Regresi√≥n lineal (baseline) | 1‚ÄØ327‚ÄØ552 | 1‚ÄØ856‚ÄØ836 |\n",
    "\n",
    "**LightGBM supera al baseline** en un ~12‚ÄØ% de MAE y ~10‚ÄØ% de RMSE, lo que\n",
    "confirma que las relaciones no‚Äëlineales y las interacciones entre lags y\n",
    "variables de calendario capturadas por los √°rboles aportan valor.\n",
    "\n",
    "### Recomendaciones inmediatas\n",
    "\n",
    "1. **Guardar modelos**  \n",
    "   ```python\n",
    "   import pickle, joblib\n",
    "   pickle.dump(best_fc, open(\"lightgbm_forecaster.pkl\",\"wb\"))\n",
    "   joblib.dump(pipe, \"baseline_linear.pkl\")\n",
    "   ```\n",
    "\n",
    "2. **Hiperpar√°metros**  \n",
    "   *Probar `min_child_samples`, `lambda_l1/l2`, `num_leaves` m√°s altos,\n",
    "   y lags adicionales `[2,3,21,30]`.*\n",
    "\n",
    "3. **Variables ex√≥genas**  \n",
    "   Incluir festivos regionales, campa√±as promocionales y clima para cada\n",
    "   tienda puede reducir error en picos.\n",
    "\n",
    "4. **Validaci√≥n rolling**  \n",
    "   Realizar back‚Äëtesting con ventanas deslizantes (p.‚ÄØej. 3√ó‚ÄØ90‚ÄØd√≠as)\n",
    "   para garantizar estabilidad a lo largo de 2024‚Äë2025.\n",
    "\n",
    "5. **Despliegue**  \n",
    "   Montar un job diario (Airflow o similar) que:\n",
    "   1. Actualice el calendario.\n",
    "   2. Cargue el modelo.\n",
    "   3. Genere la previsi√≥n del d√≠a + 14.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8400b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Buscando mejores hiperpar√°metros‚Ä¶\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[01] RMSE VALID: 1,688,084  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[02] RMSE VALID: 1,712,474  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[03] RMSE VALID: 1,681,751  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[04] RMSE VALID: 1,677,284  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[05] RMSE VALID: 1,645,356  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[06] RMSE VALID: 1,649,776  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[07] RMSE VALID: 1,646,151  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[08] RMSE VALID: 1,617,421  |  {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[09] RMSE VALID: 1,752,011  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[10] RMSE VALID: 1,755,486  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[11] RMSE VALID: 1,724,857  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[12] RMSE VALID: 1,744,285  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[13] RMSE VALID: 1,692,321  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[14] RMSE VALID: 1,695,237  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[15] RMSE VALID: 1,690,217  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[16] RMSE VALID: 1,687,158  |  {'n_estimators': 300, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[17] RMSE VALID: 1,638,980  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[18] RMSE VALID: 1,668,554  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[19] RMSE VALID: 1,630,117  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[20] RMSE VALID: 1,620,819  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[21] RMSE VALID: 1,613,393  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[22] RMSE VALID: 1,603,816  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[23] RMSE VALID: 1,600,540  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[24] RMSE VALID: 1,583,994  |  {'n_estimators': 600, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[25] RMSE VALID: 1,683,067  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[26] RMSE VALID: 1,681,123  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[27] RMSE VALID: 1,664,001  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[28] RMSE VALID: 1,670,643  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[29] RMSE VALID: 1,637,227  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[30] RMSE VALID: 1,629,601  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[31] RMSE VALID: 1,630,275  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[32] RMSE VALID: 1,617,912  |  {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[33] RMSE VALID: 1,612,838  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[34] RMSE VALID: 1,636,733  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[35] RMSE VALID: 1,600,254  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[36] RMSE VALID: 1,596,193  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[37] RMSE VALID: 1,597,742  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[38] RMSE VALID: 1,593,139  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39] RMSE VALID: 1,588,915  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[40] RMSE VALID: 1,575,469  |  {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[41] RMSE VALID: 1,647,453  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[42] RMSE VALID: 1,653,610  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[43] RMSE VALID: 1,632,324  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[44] RMSE VALID: 1,643,041  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 31, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[45] RMSE VALID: 1,616,765  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[46] RMSE VALID: 1,613,762  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': -1, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[47] RMSE VALID: 1,605,769  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1321\n",
      "[LightGBM] [Info] Number of data points in the train set: 133891, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2671624.256858\n",
      "[48] RMSE VALID: 1,593,948  |  {'n_estimators': 900, 'learning_rate': 0.03, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "\n",
      "‚úÖ Mejores hiperpar√°metros: {'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 63, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "RMSE VALID: 1,575,469\n",
      "Tiempo b√∫squeda: 6.5 min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1322\n",
      "[LightGBM] [Info] Number of data points in the train set: 186225, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 2628835.930520\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6 ¬∑ Grid search manual y predicci√≥n TEST\n",
    "# ============================================================\n",
    "\n",
    "from itertools import product\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np, pandas as pd, time\n",
    "\n",
    "# ---------- Configuraci√≥n -----------------------------------\n",
    "param_grid = {\n",
    "    'n_estimators'  : [300, 600, 900],\n",
    "    'learning_rate' : [0.05, 0.03],\n",
    "    'num_leaves'    : [31, 63],\n",
    "    'max_depth'     : [-1, 8],\n",
    "    'subsample'     : [0.8],\n",
    "    'colsample_bytree':[1.0, 0.8]\n",
    "}\n",
    "lags_used = [1, 7, 14, 28]        # lags fijos\n",
    "valid_steps = len(y_valid)        # tama√±o ventana VALID\n",
    "\n",
    "def param_combos(grid):\n",
    "    keys, vals = zip(*grid.items())\n",
    "    for combo in product(*vals):\n",
    "        yield dict(zip(keys, combo))\n",
    "\n",
    "results = []\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"‚è≥ Buscando mejores hiperpar√°metros‚Ä¶\")\n",
    "for i, params in enumerate(param_combos(param_grid), 1):\n",
    "    lgb = LGBMRegressor(objective='regression', random_state=42,\n",
    "                        n_jobs=-1, **params)\n",
    "\n",
    "    fore = ForecasterRecursiveMultiSeries(regressor=lgb, lags=lags_used)\n",
    "    fore.fit(series=y_train, exog=X_train)\n",
    "\n",
    "    # --- predicci√≥n VALID ---\n",
    "    pred_long = fore.predict(steps=valid_steps, exog=X_valid)\n",
    "\n",
    "    # Formato largo ‚Üí ancho\n",
    "    if list(pred_long.columns) == ['level', 'pred']:\n",
    "        pred = (pred_long.pivot(columns='level', values='pred')\n",
    "                          .astype('float32'))\n",
    "        pred.index = y_valid.index\n",
    "    else:\n",
    "        pred = (pred_long.groupby(level=0, axis=1).first()\n",
    "                if isinstance(pred_long.columns, pd.MultiIndex)\n",
    "                else pred_long)\n",
    "\n",
    "    # Alinear nombres\n",
    "    pred.columns    = pred.columns.astype(str)\n",
    "    y_valid.columns = y_valid.columns.astype(str)\n",
    "    common = y_valid.columns.intersection(pred.columns)\n",
    "    rmse   = np.sqrt(mean_squared_error(\n",
    "                y_valid[common].values.flatten(),\n",
    "                pred[common].values.flatten()))\n",
    "    results.append({'params': params, 'rmse': rmse})\n",
    "    print(f\"[{i:02d}] RMSE VALID: {rmse:,.0f}  |  {params}\")\n",
    "\n",
    "best = min(results, key=lambda d: d['rmse'])\n",
    "best_params = best['params']\n",
    "print(f\"\\n‚úÖ Mejores hiperpar√°metros: {best_params}\")\n",
    "print(f\"RMSE VALID: {best['rmse']:,.0f}\")\n",
    "print(f\"Tiempo b√∫squeda: {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "# ============================================================\n",
    "# 7 ¬∑ Re-entrenar con train+valid  y predecir TEST\n",
    "# ============================================================\n",
    "\n",
    "y_train_val = pd.concat([y_train, y_valid])\n",
    "X_train_val = calendar(y_train_val.index)\n",
    "\n",
    "best_lgb = LGBMRegressor(objective='regression', random_state=42,\n",
    "                         n_jobs=-1, **best_params)\n",
    "\n",
    "best_fc = ForecasterRecursiveMultiSeries(regressor=best_lgb, lags=lags_used)\n",
    "best_fc.fit(series=y_train_val, exog=X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6c7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TEST MAE  : 1,770,046\n",
      "üìä TEST RMSE : 2,991,042\n",
      "üìÅ Archivo guardado: predicciones_test_tienda.csv\n"
     ]
    }
   ],
   "source": [
    "# --- preparar TEST ---\n",
    "test_start = valid_end + pd.Timedelta(days=1)\n",
    "test_end   = y_wide.index.max()\n",
    "y_test     = y_wide.loc[test_start:test_end]\n",
    "X_test     = calendar(y_test.index)\n",
    "\n",
    "# --- predicci√≥n TEST ---\n",
    "pred_long = best_fc.predict(steps=len(y_test), exog=X_test)\n",
    "\n",
    "if list(pred_long.columns) == ['level', 'pred']:\n",
    "    pred_test = (pred_long.pivot(columns='level', values='pred')\n",
    "                          .astype('float32'))\n",
    "    pred_test.index = y_test.index\n",
    "else:\n",
    "    pred_test = (pred_long.groupby(level=0, axis=1).first()\n",
    "                 if isinstance(pred_long.columns, pd.MultiIndex)\n",
    "                 else pred_long)\n",
    "\n",
    "pred_test.columns = pred_test.columns.astype(str)\n",
    "y_test.columns    = y_test.columns.astype(str)\n",
    "common = y_test.columns.intersection(pred_test.columns)\n",
    "\n",
    "mae_test  = mean_absolute_error(\n",
    "                y_test[common].values.flatten(),\n",
    "                pred_test[common].values.flatten())\n",
    "rmse_test = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    y_test[common].values.flatten(),\n",
    "                    pred_test[common].values.flatten()))\n",
    "\n",
    "print(f\"\\nüìä TEST MAE  : {mae_test:,.0f}\")\n",
    "print(f\"üìä TEST RMSE : {rmse_test:,.0f}\")\n",
    "\n",
    "# --- guardar predicciones ---\n",
    "pred_test.to_csv(\"predicciones_test_tienda.csv\")\n",
    "print(\"üìÅ Archivo guardado: predicciones_test_tienda.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf4401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReg VALID  MAE : 1,327,552 | RMSE : 1,856,836\n"
     ]
    }
   ],
   "source": [
    "# 6 ¬∑ Baseline Regresi√≥n lineal\n",
    "long_pdf = df.filter(F.col('FechaVenta') <= F.lit(valid_end.strftime('%Y-%m-%d'))).toPandas()\n",
    "long_pdf['FechaVenta'] = pd.to_datetime(long_pdf['FechaVenta'])\n",
    "for col, func in {'dow':'dayofweek','day':'day','month':'month','year':'year'}.items():\n",
    "    long_pdf[col] = getattr(long_pdf['FechaVenta'].dt, func).astype('int16')\n",
    "long_pdf['weekend'] = (long_pdf['dow']>=5).astype('int8')\n",
    "long_pdf['weekofyear'] = long_pdf['FechaVenta'].dt.isocalendar().week.astype('int8')\n",
    "\n",
    "mask_tr = long_pdf['FechaVenta'] <= train_end\n",
    "mask_val= (long_pdf['FechaVenta'] > train_end) & (long_pdf['FechaVenta'] <= valid_end)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_cols = ['Tienda','dow','weekend','day','month','year','weekofyear']\n",
    "y_col  = 'PrecioVta'\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)  # ‚Üê cambio\n",
    "prep = ColumnTransformer(\n",
    "        transformers=[('cat', ohe, ['Tienda'])],\n",
    "        remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', prep),\n",
    "    ('reg',  LinearRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(long_pdf.loc[mask_tr, X_cols], long_pdf.loc[mask_tr, y_col])\n",
    "pred_lin = pipe.predict(long_pdf.loc[mask_val, X_cols])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "mae_lin = mean_absolute_error(long_pdf.loc[mask_val, y_col], pred_lin)\n",
    "rmse_lin = np.sqrt(mean_squared_error(long_pdf.loc[mask_val, y_col], pred_lin))\n",
    "print(f\"LinearReg VALID  MAE : {mae_lin:,.0f} | RMSE : {rmse_lin:,.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "320e9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
